{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Spark Program on Employee Dataset\n",
        "\n",
        "Dataset contains employee information.\n",
        "Assume the dataset includes a Salary column (or Salary derived from PaymentTier)."
      ],
      "metadata": {
        "id": "daAnqJg99YTu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIW0L7pF9S4I",
        "outputId": "cd83d867-bb14-4d0b-c367-0c92623097e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+---------------+------+-------+-----------------+---------------+------------------+\n",
            "|First Name|Gender|Start Date|Last Login Time|Salary|Bonus %|Senior Management|           Team|     UpdatedSalary|\n",
            "+----------+------+----------+---------------+------+-------+-----------------+---------------+------------------+\n",
            "| Katherine|Female| 8/13/1996|       12:21 AM|149908| 18.912|            false|        Finance|164898.80000000002|\n",
            "|      Rose|Female| 5/28/2015|        8:40 AM|149903|   5.63|            false|Human Resources|164893.30000000002|\n",
            "|   Cynthia|Female| 7/12/2006|        8:55 AM|149684|  7.864|            false|        Product|164652.40000000002|\n",
            "|      NULL|Female| 2/23/2005|        9:50 PM|149654|  1.825|             NULL|          Sales|164619.40000000002|\n",
            "|     Kathy|Female| 3/18/2000|        7:26 PM|149563| 16.991|             true|        Finance|164519.30000000002|\n",
            "+----------+------+----------+---------------+------+-------+-----------------+---------------+------------------+\n",
            "only showing top 5 rows\n",
            "Total qualifying employees: 854\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"EmployeeProcessing\").getOrCreate()\n",
        "\n",
        "# Read CSV file\n",
        "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Filter employees with salary > 50,000\n",
        "filtered_df = df.filter(col(\"Salary\") > 50000)\n",
        "\n",
        "# Increase salary by 10%\n",
        "updated_df = filtered_df.withColumn(\n",
        "    \"UpdatedSalary\",\n",
        "    col(\"Salary\") * 1.10\n",
        ")\n",
        "\n",
        "# Show top 5 highest salaries\n",
        "updated_df.orderBy(col(\"UpdatedSalary\").desc()).show(5)\n",
        "\n",
        "# Count qualifying employees\n",
        "count = updated_df.count()\n",
        "print(\"Total qualifying employees:\", count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Pair RDD Operations"
      ],
      "metadata": {
        "id": "OPO0zVDJgvVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Data\n",
        "data = [(\"A\",10), (\"B\",20), (\"A\",30), (\"B\",40), (\"C\",50)]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# (a) Total value per key\n",
        "total_per_key = rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# (b) Average value per key\n",
        "avg_per_key = rdd.mapValues(lambda x: (x,1)) \\\n",
        "                 .reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1])) \\\n",
        "                 .mapValues(lambda x: x[0]/x[1])\n",
        "\n",
        "# (c) Sorted by key\n",
        "avg_per_key.sortByKey().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh_4RE7w-muw",
        "outputId": "deb94bf5-ddd0-4fe8-cde2-abac05268674"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 20.0), ('B', 30.0), ('C', 50.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Department Marks using Spark\n"
      ],
      "metadata": {
        "id": "zd53r8aFg96G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Format\n",
        "data = [(\"CS\",80), (\"AI\",90), (\"IT\",70), (\"IT\",85), (\"EE\",75)]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "# (a) Max marks per department\n",
        "max_marks = rdd.reduceByKey(lambda x, y: max(x, y))\n",
        "\n",
        "# (b) Average marks per department\n",
        "avg_marks = rdd.mapValues(lambda x:(x,1)) \\\n",
        "               .reduceByKey(lambda a,b:(a[0]+b[0], a[1]+b[1])) \\\n",
        "               .mapValues(lambda x:x[0]/x[1])\n",
        "\n",
        "# (c) Departments with average > 75\n",
        "result = avg_marks.filter(lambda x: x[1] > 75)\n",
        "\n",
        "# Actions to display output\n",
        "print(\"MAX MARKS:\", max_marks.collect())\n",
        "print(\"AVG MARKS:\", avg_marks.collect())\n",
        "print(\"RESULT:\", result.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiptwy1X9bbe",
        "outputId": "a8296e71-68da-4ad5-d173-bc4f75109972"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAX MARKS: [('CS', 80), ('AI', 90), ('IT', 85), ('EE', 75)]\n",
            "AVG MARKS: [('CS', 80.0), ('AI', 90.0), ('IT', 77.5), ('EE', 75.0)]\n",
            "RESULT: [('CS', 80.0), ('AI', 90.0), ('IT', 77.5)]\n"
          ]
        }
      ]
    }
  ]
}