{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FgFApCFqyT4V"
      },
      "outputs": [],
      "source": [
        "dataset_path = os.path.join(path, \"IDC_regular_ps50_idx5\")\n",
        "\n",
        "limit = 15000\n",
        "count = 0\n",
        "\n",
        "for patient in os.listdir(dataset_path):\n",
        "\n",
        "    patient_path = os.path.join(dataset_path, patient)\n",
        "\n",
        "    for label in [\"0\", \"1\"]:\n",
        "\n",
        "        folder = os.path.join(patient_path, label)\n",
        "\n",
        "        if not os.path.exists(folder):\n",
        "            continue\n",
        "\n",
        "        for img_name in os.listdir(folder):\n",
        "\n",
        "            if count >= limit:\n",
        "                break\n",
        "\n",
        "            img_path = os.path.join(folder, img_name)\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            img = img / 255.0\n",
        "\n",
        "            data.append(img)\n",
        "            labels.append(int(label))\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        if count >= limit:\n",
        "            break\n",
        "\n",
        "    if count >= limit:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "IMG_SIZE = 50\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "dataset_path = os.path.join(path, \"IDC_regular_ps50_idx5\")\n",
        "\n",
        "limit = 15000\n",
        "count = 0\n",
        "\n",
        "for patient in os.listdir(dataset_path):\n",
        "\n",
        "    patient_path = os.path.join(dataset_path, patient)\n",
        "\n",
        "    for label in [\"0\", \"1\"]:\n",
        "\n",
        "        folder = os.path.join(patient_path, label)\n",
        "\n",
        "        if not os.path.exists(folder):\n",
        "            continue\n",
        "\n",
        "        for img_name in os.listdir(folder):\n",
        "\n",
        "            if count >= limit:\n",
        "                break\n",
        "\n",
        "            img_path = os.path.join(folder, img_name)\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "            img = img / 255.0\n",
        "\n",
        "            data.append(img)\n",
        "            labels.append(int(label))\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        if count >= limit:\n",
        "            break\n",
        "\n",
        "    if count >= limit:\n",
        "        break\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(\"Loaded images:\", len(data))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_fGYCHwyXGH",
        "outputId": "1b5992f2-c9e5-40a6-ebb2-c5edf65544c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded images: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Flatten images\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_flat, y_train)\n",
        "print(\"LR Accuracy:\", accuracy_score(y_test, lr.predict(X_test_flat)))\n",
        "\n",
        "# KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_flat, y_train)\n",
        "print(\"KNN Accuracy:\", accuracy_score(y_test, knn.predict(X_test_flat)))\n",
        "\n",
        "# SVM\n",
        "svm = SVC()\n",
        "svm.fit(X_train_flat, y_train)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm.predict(X_test_flat)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90hzveTuyefL",
        "outputId": "6fa50199-9c60-43d8-f411-44a3ca66e00f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Accuracy: 0.7946666666666666\n",
            "KNN Accuracy: 0.8116666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "\n",
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Conv2D(32, (3,3), activation='relu', input_shape=(50,50,3)))\n",
        "model1.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model1.add(MaxPooling2D((2,2)))\n",
        "model1.add(Dropout(0.25))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(128, activation='relu'))\n",
        "model1.add(Dropout(0.25))\n",
        "model1.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model1.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model1.fit(X_train, y_train, epochs=12, batch_size=128,\n",
        "           validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "VsEdEMBNyhCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Conv2D(32,(3,3),activation='relu',padding='same',input_shape=(50,50,3)))\n",
        "model2.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n",
        "model2.add(MaxPooling2D((2,2)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.25))\n",
        "\n",
        "model2.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
        "model2.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n",
        "model2.add(MaxPooling2D((2,2)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.25))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(512,activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "               loss='sparse_categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "model2.fit(X_train, y_train, epochs=12, batch_size=128,\n",
        "           validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "HTUbvj9Cyjf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Conv2D(32,(5,5),activation='relu',input_shape=(50,50,3)))\n",
        "model3.add(MaxPooling2D((3,3)))\n",
        "\n",
        "model3.add(Conv2D(32,(5,5),activation='relu'))\n",
        "model3.add(MaxPooling2D((3,3)))\n",
        "\n",
        "model3.add(Dropout(0.25))\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(64,activation='relu'))\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(2,activation='softmax'))\n",
        "\n",
        "model3.compile(optimizer='adam',\n",
        "               loss='sparse_categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "\n",
        "model3.fit(X_train, y_train, epochs=12, batch_size=128,\n",
        "           validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "EyE5Da6ayl8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "pred = model3.predict(X_test)\n",
        "pred = np.argmax(pred, axis=1)\n",
        "\n",
        "print(classification_report(y_test, pred))\n",
        "print(confusion_matrix(y_test, pred))\n"
      ],
      "metadata": {
        "id": "aeo45q6nyods"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhPcrSOHyrXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}